# ADR-004: 5축 벡터 검색 전략

## 상태

Accepted

## 배경

NL2SQL 파이프라인에서 사용자 질문에 관련된 테이블/컬럼을 찾아야 한다. 검색 정확도가 SQL 생성 품질에 직접적으로 영향을 미치므로, 검색 전략의 선택이 매우 중요하다.

검색 전략 후보:

1. **단일 벡터 검색**: 질문 임베딩만으로 검색
2. **5축 벡터 검색**: question, HyDE, regex, intent, PRF 5가지 축으로 검색
3. **2축 검색**: 벡터 + 키워드만 사용
4. **LLM 직접 선택**: LLM에게 스키마 전체를 보여주고 관련 테이블 선택 요청

## 고려한 옵션

### 옵션 A: 단일 벡터 검색

**장점**:
- 구현 단순
- LLM 호출 불필요 (임베딩만)
- 빠른 실행 속도

**단점**:
- 자연어 질문과 DB 스키마 간 **의미 격차(semantic gap)** 해소 불충분
  - "매출 현황" → `sales_records` 테이블을 찾지 못할 수 있음
- 정확한 명칭 매칭 불가 (벡터 유사도의 한계)
- 질문 의도(집계/필터/비교)를 반영하지 못함

### 옵션 B: 5축 벡터 검색 (K-AIR 방식)

**장점**:
- **의미 격차 해소**: HyDE가 질문과 스키마 사이의 의미적 다리 역할
- **정확 매칭**: regex 축이 정확한 테이블/컬럼 이름 매칭
- **의도 반영**: intent 축이 쿼리 패턴(집계/필터 등) 반영
- **정밀도 개선**: PRF가 초기 결과를 기반으로 쿼리 보강
- **결과 융합**: RRF 알고리즘으로 다양한 관점의 결과를 통합

**단점**:
- **LLM 호출 비용**: HyDE, intent 생성에 LLM 호출 필요 (2회)
- **지연 시간**: 5축 검색으로 인한 추가 지연 (~500ms)
- **복잡도**: 구현 및 디버깅 복잡

### 옵션 C: 2축 검색 (벡터 + 키워드)

**장점**:
- 5축보다 단순하면서 단일 축보다 정확
- LLM 호출 불필요

**단점**:
- HyDE 없이 의미 격차 해소 불충분
- 의도 반영 불가

### 옵션 D: LLM 직접 선택

**장점**:
- LLM의 높은 이해 능력 활용
- 구현 단순 (프롬프트에 스키마 전체 포함)

**단점**:
- **스키마 크기 제한**: 수십~수백 테이블의 DDL을 프롬프트에 넣으면 토큰 한계 초과
- **비용**: 매 요청마다 대량 토큰 소비
- **비결정성**: LLM 응답의 비결정성이 검색 품질을 불안정하게 만듦
- 스키마 변경 시 프롬프트 자동 갱신 필요

## 선택한 결정

**옵션 B: 5축 벡터 검색**

## 근거

1. **K-AIR 운영 검증**: K-AIR에서 운영 데이터(~100개 테이블)에 대해 5축 벡터 검색을 운영하며, 단일 축 대비 recall이 30% 이상 향상됨을 확인.

2. **의미 격차 해소**: HyDE 축이 "매출 현황"이라는 질문에서 "sales_records 테이블의 revenue를 SUM하는 쿼리"라는 중간 설명을 생성하여, 질문 벡터와 스키마 벡터 사이의 의미 격차를 효과적으로 메움.

3. **비용 대비 효과**: HyDE와 intent 생성에 gpt-4o-mini를 사용하면 호출당 ~$0.001 수준. SQL 생성 정확도 향상 대비 합리적인 비용.

4. **병렬 실행**: 5축 검색을 `asyncio.gather()`로 병렬 실행하면 추가 지연은 최대 축의 지연 시간과 같아 ~500ms 수준.

5. **RRF 융합**: Reciprocal Rank Fusion은 여러 검색 결과를 통합하는 검증된 알고리즘으로, 어느 한 축의 실패가 전체 결과를 망치지 않는 강건성을 제공.

## 결과

### 긍정적 영향

- NL2SQL 정확도 향상 (관련 테이블 검색 precision/recall 개선)
- 복합 질문에서의 관련 테이블 탐지율 향상
- K-AIR 검증 코드 재활용

### 부정적 영향

- LLM 호출 비용 증가 (요청당 ~$0.002 추가)
- 구현 복잡도 증가
- HyDE/intent 생성 실패 시 해당 축 결과 누락

### 완화 전략

- HyDE/intent에 gpt-4o-mini 사용하여 비용 절감
- 각 축의 실패를 독립적으로 처리 (한 축 실패 시 나머지 축으로 검색 계속)
- 검색 결과를 Redis에 단기 캐싱 (동일 질문 반복 시 재검색 방지)

## 재평가 조건

- LLM 호출 비용이 문제가 될 때 (월간 비용 모니터링)
- 단일/2축 검색으로도 충분한 정확도가 나올 때
- 전문 벡터 DB(Pinecone 등)의 하이브리드 검색이 더 나은 성능을 보일 때

---

**근거 문서**: [01_architecture/nl2sql-pipeline.md](../01_architecture/nl2sql-pipeline.md)
**영향 문서**: [03_backend/graph-search.md](../03_backend/graph-search.md), [05_llm/llm-factory.md](../05_llm/llm-factory.md)
